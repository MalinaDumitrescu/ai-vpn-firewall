{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6475e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total flows: 33711\n",
      "Excluded flows (pkt_len > 5000): 371 (1.1005%)\n",
      "Safe ranges to read from H5: 302\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ab399f0b95f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mr_stop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mCHUNK_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_stop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[0mdf_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlocal_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_chunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\scoti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, key, where, start, stop, columns, iterator, chunksize, auto_close)\u001b[0m\n\u001b[0;32m    917\u001b[0m         )\n\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 919\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    920\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m     def select_as_coordinates(\n",
      "\u001b[1;32mc:\\Users\\scoti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self, coordinates)\u001b[0m\n\u001b[0;32m   2040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2041\u001b[0m         \u001b[1;31m# directly return the result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2042\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2043\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2044\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\scoti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36mfunc\u001b[1;34m(_start, _stop, _where)\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;31m# function to call on iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_where\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_where\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    904\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m         \u001b[1;31m# create the iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\scoti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, where, columns, start, stop)\u001b[0m\n\u001b[0;32m   3357\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3358\u001b[0m             \u001b[0mblk_items\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"block{i}_items\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3359\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"block{i}_values\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_stop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3361\u001b[0m             \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk_items\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\scoti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36mread_array\u001b[1;34m(self, key, start, stop)\u001b[0m\n\u001b[0;32m   2981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2982\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVLArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2983\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2984\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"value_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\scoti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tables\\vlarray.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    686\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_range\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m             start, stop, step = self._process_range(\n",
      "\u001b[1;32mc:\\Users\\scoti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tables\\vlarray.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, start, stop, step)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[0matom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0matom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"size\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# it is a pseudo-atom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m             \u001b[0moutlistarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0matom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlistarr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m             \u001b[1;31m# Convert the list to the right flavor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\scoti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tables\\vlarray.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[0matom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0matom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"size\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# it is a pseudo-atom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m             \u001b[0moutlistarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0matom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlistarr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m             \u001b[1;31m# Convert the list to the right flavor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\scoti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tables\\atom.py\u001b[0m in \u001b[0;36mfromarray\u001b[1;34m(self, array)\u001b[0m\n\u001b[0;32m   1372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--vnat_h5\", type=str, required=True)\n",
    "    ap.add_argument(\"--flows_parquet\", type=str, required=True)\n",
    "    ap.add_argument(\"--train_caps\", type=str, required=True)  # vnat_train_captures.txt\n",
    "    ap.add_argument(\"--out_dir\", type=str, required=True)\n",
    "\n",
    "    ap.add_argument(\"--N\", type=int, default=100)\n",
    "    ap.add_argument(\"--MIN_K\", type=int, default=50)\n",
    "    ap.add_argument(\"--max_raw\", type=int, default=0, help=\"0 disables cap; else cap raw packets per flow (e.g., 5000)\")\n",
    "    ap.add_argument(\"--h5_key\", type=str, default=\"\", help=\"H5 key; leave empty to auto-pick first key\")\n",
    "    return ap.parse_args()\n",
    "\n",
    "\n",
    "class RunningStats:\n",
    "    \"\"\"Running mean/std via sums. Stable and fast.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.sum = 0.0\n",
    "        self.sumsq = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, vals: np.ndarray):\n",
    "        v = vals.astype(np.float64, copy=False).ravel()\n",
    "        if v.size == 0:\n",
    "            return\n",
    "        self.sum += float(v.sum())\n",
    "        self.sumsq += float((v * v).sum())\n",
    "        self.count += int(v.size)\n",
    "\n",
    "    def finalize(self):\n",
    "        if self.count <= 1:\n",
    "            return 0.0, 1.0\n",
    "        mu = self.sum / self.count\n",
    "        var = max((self.sumsq / self.count) - (mu * mu), 0.0)\n",
    "        sigma = float(np.sqrt(var))\n",
    "        if sigma == 0.0:\n",
    "            sigma = 1.0\n",
    "        return float(mu), float(sigma)\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    VNAT_H5 = Path(args.vnat_h5)\n",
    "    FLOWS = Path(args.flows_parquet)\n",
    "    TRAIN_CAPS_TXT = Path(args.train_caps)\n",
    "    OUT_DIR = Path(args.out_dir)\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    N = int(args.N)\n",
    "    MIN_K = int(args.MIN_K)\n",
    "    MAX_RAW = int(args.max_raw)\n",
    "\n",
    "    OUT_X = OUT_DIR / \"cnn_X.npy\"\n",
    "    OUT_C = OUT_DIR / \"cnn_count.npy\"\n",
    "    OUT_Y = OUT_DIR / \"cnn_y.npy\"\n",
    "    OUT_K = OUT_DIR / \"cnn_k.npy\"\n",
    "    OUT_STATS = OUT_DIR / \"normalization_stats.json\"\n",
    "\n",
    "    # --- load flows + train split ---\n",
    "    flows = pd.read_parquet(FLOWS).set_index(\"flow_id\").sort_index()\n",
    "    num_flows = len(flows)\n",
    "\n",
    "    train_caps = set(TRAIN_CAPS_TXT.read_text(encoding=\"utf-8\").splitlines())\n",
    "    is_train_flow = flows[\"capture_id\"].isin(train_caps).to_numpy()\n",
    "\n",
    "    Y = flows[\"label\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "    # --- allocate outputs ---\n",
    "    X = np.zeros((num_flows, N, 2), dtype=np.float32)\n",
    "    C = np.zeros((num_flows,), dtype=np.float32)\n",
    "    K = np.zeros((num_flows,), dtype=np.int32)\n",
    "\n",
    "    rs_size = RunningStats()\n",
    "    rs_iat = RunningStats()\n",
    "\n",
    "    # --- read entire VNAT H5 (server RAM should handle) ---\n",
    "    # If you still want safety, you can use HDFStore + select, but on server itâ€™s usually fine.\n",
    "    if args.h5_key.strip():\n",
    "        key = args.h5_key.strip()\n",
    "        df = pd.read_hdf(VNAT_H5, key=key)\n",
    "    else:\n",
    "        with pd.HDFStore(VNAT_H5, mode=\"r\") as store:\n",
    "            keys = store.keys()\n",
    "            if not keys:\n",
    "                raise ValueError(\"H5 file has no keys.\")\n",
    "            key = keys[0]\n",
    "        df = pd.read_hdf(VNAT_H5, key=key)\n",
    "\n",
    "    # align row i with flow_id i\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    if len(df) != num_flows:\n",
    "        raise ValueError(f\"Mismatch: H5 rows={len(df)} vs flows={num_flows}. \"\n",
    "                         f\"Your flow_id alignment assumption breaks.\")\n",
    "\n",
    "    # --- build X/C/K and collect train stats ---\n",
    "    for flow_id in range(num_flows):\n",
    "        ts = np.asarray(df.at[flow_id, \"timestamps\"], dtype=np.float64)\n",
    "        sz = np.asarray(df.at[flow_id, \"sizes\"], dtype=np.float64)\n",
    "        dr = np.asarray(df.at[flow_id, \"directions\"], dtype=np.int64)\n",
    "\n",
    "        if ts.size == 0:\n",
    "            continue\n",
    "\n",
    "        if MAX_RAW > 0 and ts.size > MAX_RAW:\n",
    "            ts = ts[:MAX_RAW]\n",
    "            sz = sz[:MAX_RAW]\n",
    "            dr = dr[:MAX_RAW]\n",
    "\n",
    "        # sort by timestamp\n",
    "        order = np.argsort(ts)\n",
    "        ts, sz, dr = ts[order], sz[order], dr[order]\n",
    "\n",
    "        k = int(min(sz.size, N))\n",
    "        K[flow_id] = k\n",
    "        C[flow_id] = k / float(N)\n",
    "\n",
    "        # signed packet size\n",
    "        signs = np.where(dr[:k] == 1, 1.0, -1.0)\n",
    "        signed_sz = signs * sz[:k]\n",
    "\n",
    "        # IAT\n",
    "        iat = np.zeros(k, dtype=np.float64)\n",
    "        if k > 1:\n",
    "            iat[1:] = np.diff(ts[:k])\n",
    "            iat[iat < 0] = 0.0\n",
    "\n",
    "        # transforms\n",
    "        size_feat = np.sign(signed_sz) * np.log1p(np.abs(signed_sz))\n",
    "        iat_feat = np.log1p(iat)\n",
    "\n",
    "        X[flow_id, :k, 0] = size_feat.astype(np.float32)\n",
    "        X[flow_id, :k, 1] = iat_feat.astype(np.float32)\n",
    "\n",
    "        # train-only stats, only if k>=MIN_K\n",
    "        if is_train_flow[flow_id] and (k >= MIN_K):\n",
    "            rs_size.update(X[flow_id, :k, 0])\n",
    "            rs_iat.update(X[flow_id, :k, 1])\n",
    "\n",
    "        if (flow_id + 1) % 2000 == 0:\n",
    "            print(f\"Processed {flow_id+1}/{num_flows}\")\n",
    "\n",
    "    mu_size, sigma_size = rs_size.finalize()\n",
    "    mu_iat, sigma_iat = rs_iat.finalize()\n",
    "\n",
    "    train_valid = int(np.sum(is_train_flow & (K >= MIN_K)))\n",
    "    print(\"Train flows with k>=MIN_K:\", train_valid)\n",
    "    print(\"Norm stats:\", {\"mu_size\": mu_size, \"sigma_size\": sigma_size, \"mu_iat\": mu_iat, \"sigma_iat\": sigma_iat})\n",
    "\n",
    "    # --- normalize globally using train stats ---\n",
    "    X[:, :, 0] = (X[:, :, 0] - mu_size) / sigma_size\n",
    "    X[:, :, 1] = (X[:, :, 1] - mu_iat) / sigma_iat\n",
    "\n",
    "    # --- save ---\n",
    "    np.save(OUT_X, X)\n",
    "    np.save(OUT_C, C)\n",
    "    np.save(OUT_Y, Y)\n",
    "    np.save(OUT_K, K)\n",
    "\n",
    "    stats = {\n",
    "        \"N\": N,\n",
    "        \"MIN_K_for_norm_stats\": MIN_K,\n",
    "        \"MAX_RAW\": MAX_RAW,\n",
    "        \"h5_key_used\": key,\n",
    "        \"mu\": {\"size\": mu_size, \"iat\": mu_iat},\n",
    "        \"sigma\": {\"size\": sigma_size, \"iat\": sigma_iat},\n",
    "        \"count_norm\": \"pkt_count_observed / N\",\n",
    "        \"shapes\": {\"X\": list(X.shape), \"C\": [int(C.shape[0])], \"Y\": [int(Y.shape[0])], \"K\": [int(K.shape[0])]},\n",
    "    }\n",
    "    OUT_STATS.write_text(json.dumps(stats, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    print(\"Saved:\")\n",
    "    print(\" \", OUT_X)\n",
    "    print(\" \", OUT_C)\n",
    "    print(\" \", OUT_Y)\n",
    "    print(\" \", OUT_K)\n",
    "    print(\" \", OUT_STATS)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f30f40",
   "metadata": {},
   "source": [
    "- In case I cannot access better resources for running the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cede22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500/33711 | bad_rows=421 | capped_monster=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001EAF48555D0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\scoti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 781, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "VNAT_H5 = Path(\"../data/raw/vnat/VNAT_Dataframe_release_1.h5\")\n",
    "FLOWS   = Path(\"../data/processed/vnat/flows.parquet\")\n",
    "\n",
    "OUT_DIR = Path(\"../data/processed/vnat\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "N = 100\n",
    "MIN_K = 50\n",
    "MAX_RAW = 5000  # cap to prevent VL allocation blowups on monster flows\n",
    "\n",
    "OUT_X = OUT_DIR / \"cnn_X.npy\"\n",
    "OUT_C = OUT_DIR / \"cnn_count.npy\"\n",
    "OUT_Y = OUT_DIR / \"cnn_y.npy\"\n",
    "OUT_K = OUT_DIR / \"cnn_k.npy\"\n",
    "\n",
    "flows = pd.read_parquet(FLOWS).set_index(\"flow_id\").sort_index()\n",
    "num_flows = len(flows)\n",
    "Y = flows[\"label\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "train_caps = set(Path(\"../data/splits/vnat_train_captures.txt\").read_text(encoding=\"utf-8\").splitlines())\n",
    "train_ids = set(flows.index[flows[\"capture_id\"].isin(train_caps)].to_numpy().tolist())\n",
    "\n",
    "X = np.zeros((num_flows, N, 2), dtype=np.float32)\n",
    "C = np.zeros((num_flows,), dtype=np.float32)\n",
    "K = np.zeros((num_flows,), dtype=np.int32)\n",
    "\n",
    "# running stats (Welford)\n",
    "class RunningStats:\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.mean = 0.0\n",
    "        self.M2 = 0.0\n",
    "    def update(self, arr):\n",
    "        arr = np.asarray(arr, dtype=np.float64).ravel()\n",
    "        for v in arr:\n",
    "            self.n += 1\n",
    "            delta = v - self.mean\n",
    "            self.mean += delta / self.n\n",
    "            delta2 = v - self.mean\n",
    "            self.M2 += delta * delta2\n",
    "    def finalize(self):\n",
    "        if self.n < 2:\n",
    "            return float(self.mean), 1.0\n",
    "        var = self.M2 / self.n\n",
    "        std = float(np.sqrt(var))\n",
    "        if std == 0.0:\n",
    "            std = 1.0\n",
    "        return float(self.mean), std\n",
    "\n",
    "rs_size = RunningStats()\n",
    "rs_iat  = RunningStats()\n",
    "\n",
    "bad_rows = 0\n",
    "skipped_monster = 0\n",
    "\n",
    "def safe_read_one_row(i: int):\n",
    "    # read exactly 1 row; avoids VL chunk allocation issues\n",
    "    try:\n",
    "        d = pd.read_hdf(VNAT_H5, start=i, stop=i+1)\n",
    "        if len(d) != 1:\n",
    "            return None\n",
    "        return d.iloc[0]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "for flow_id in range(num_flows):\n",
    "    row = safe_read_one_row(flow_id)\n",
    "    if row is None:\n",
    "        bad_rows += 1\n",
    "        continue\n",
    "\n",
    "    ts = np.asarray(row[\"timestamps\"], dtype=np.float64)\n",
    "    sz = np.asarray(row[\"sizes\"], dtype=np.float64)\n",
    "    dr = np.asarray(row[\"directions\"], dtype=np.int64)\n",
    "\n",
    "    if ts.size == 0:\n",
    "        continue\n",
    "\n",
    "    # cap extremely long flows early to avoid heavy ops\n",
    "    if ts.size > MAX_RAW:\n",
    "        ts = ts[:MAX_RAW]\n",
    "        sz = sz[:MAX_RAW]\n",
    "        dr = dr[:MAX_RAW]\n",
    "        skipped_monster += 1\n",
    "\n",
    "    # sort by time\n",
    "    order = np.argsort(ts)\n",
    "    ts, sz, dr = ts[order], sz[order], dr[order]\n",
    "\n",
    "    k = int(min(sz.size, N))\n",
    "    K[flow_id] = k\n",
    "    C[flow_id] = k / float(N)\n",
    "\n",
    "    # signed sizes\n",
    "    signs = np.where(dr[:k] == 1, 1.0, -1.0)\n",
    "    signed_sz = signs * sz[:k]\n",
    "\n",
    "    # IAT\n",
    "    iat = np.zeros(k, dtype=np.float64)\n",
    "    if k > 1:\n",
    "        iat[1:] = np.diff(ts[:k])\n",
    "        iat[iat < 0] = 0.0\n",
    "\n",
    "    # transforms\n",
    "    size_feat = np.sign(signed_sz) * np.log1p(np.abs(signed_sz))\n",
    "    iat_feat  = np.log1p(iat)\n",
    "\n",
    "    X[flow_id, :k, 0] = size_feat.astype(np.float32)\n",
    "    X[flow_id, :k, 1] = iat_feat.astype(np.float32)\n",
    "\n",
    "    # update normalization stats on TRAIN ONLY, and only if enough packets\n",
    "    if (flow_id in train_ids) and (k >= MIN_K):\n",
    "        rs_size.update(X[flow_id, :k, 0])\n",
    "        rs_iat.update(X[flow_id, :k, 1])\n",
    "\n",
    "    if (flow_id + 1) % 500 == 0:\n",
    "        print(f\"Processed {flow_id+1}/{num_flows} | bad_rows={bad_rows} | capped_monster={skipped_monster}\")\n",
    "\n",
    "mu_size, sigma_size = rs_size.finalize()\n",
    "mu_iat,  sigma_iat  = rs_iat.finalize()\n",
    "\n",
    "print(\"Final stats:\", mu_size, sigma_size, mu_iat, sigma_iat)\n",
    "print(\"Bad rows:\", bad_rows, \"| Monster flows capped:\", skipped_monster)\n",
    "\n",
    "# standardize\n",
    "X[:, :, 0] = (X[:, :, 0] - mu_size) / sigma_size\n",
    "X[:, :, 1] = (X[:, :, 1] - mu_iat) / sigma_iat\n",
    "\n",
    "# save\n",
    "np.save(OUT_X, X)\n",
    "np.save(OUT_C, C)\n",
    "np.save(OUT_Y, Y)\n",
    "np.save(OUT_K, K)\n",
    "\n",
    "stats = {\n",
    "    \"N\": N,\n",
    "    \"MIN_K_for_norm_stats\": MIN_K,\n",
    "    \"MAX_RAW_cap\": MAX_RAW,\n",
    "    \"mu\": {\"size\": mu_size, \"iat\": mu_iat},\n",
    "    \"sigma\": {\"size\": sigma_size, \"iat\": sigma_iat},\n",
    "    \"count_norm\": \"pkt_count_observed / N\",\n",
    "    \"bad_rows\": int(bad_rows),\n",
    "    \"monster_flows_capped\": int(skipped_monster),\n",
    "}\n",
    "\n",
    "Path(\"../artifacts/cnn\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"../artifacts/cnn/normalization_stats.json\").write_text(json.dumps(stats, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved:\", OUT_X, OUT_C, OUT_Y, OUT_K)\n",
    "print(\"X shape:\", X.shape, \"C min/max:\", float(C.min()), float(C.max()))\n",
    "print(\"Y labels:\", set(np.unique(Y).tolist()))\n",
    "print(\"Stats saved to artifacts/cnn/normalization_stats.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
