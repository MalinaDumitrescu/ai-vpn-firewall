{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe92118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captures total: 165\n",
      "Captures Train/Val/Test: 154 11 0\n",
      "Flows Train: 33249 class_counts: {0: 32876, 1: 373}\n",
      "Flows Val:   462 class_counts: {0: 456, 1: 6}\n",
      "Flows Test:  0 class_counts: {}\n",
      "Saved: ..\\data\\splits\\vnat_train_captures.txt ..\\data\\splits\\vnat_val_captures.txt ..\\data\\splits\\vnat_test_captures.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "FLOWS = Path(\"../data/processed/vnat/flows.parquet\")\n",
    "OUT_DIR = Path(\"../data/splits\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_path = OUT_DIR / \"vnat_train_captures.txt\"\n",
    "val_path   = OUT_DIR / \"vnat_val_captures.txt\"\n",
    "test_path  = OUT_DIR / \"vnat_test_captures.txt\"\n",
    "\n",
    "flows = pd.read_parquet(FLOWS)\n",
    "\n",
    "# flow counts per capture (one label per capture in VNAT, but we compute safely anyway)\n",
    "cap_stats = flows.groupby([\"capture_id\", \"label\"]).size().reset_index(name=\"n_flows\")\n",
    "\n",
    "vpn_caps = cap_stats[cap_stats[\"label\"] == 1][[\"capture_id\", \"n_flows\"]].copy()\n",
    "non_caps = cap_stats[cap_stats[\"label\"] == 0][[\"capture_id\", \"n_flows\"]].copy()\n",
    "\n",
    "# Shuffle deterministically\n",
    "vpn_caps = vpn_caps.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "non_caps = non_caps.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "def greedy_split_by_flows(df_caps: pd.DataFrame, train=0.70, val=0.15):\n",
    "    \"\"\"\n",
    "    Greedy assignment of captures to splits aiming for flow-count targets.\n",
    "    Keeps captures disjoint and balances by total number of flows, not capture count.\n",
    "    \"\"\"\n",
    "    total = float(df_caps[\"n_flows\"].sum())\n",
    "    target_train = total * train\n",
    "    target_val   = total * val\n",
    "\n",
    "    tr, va, te = [], [], []\n",
    "    s_tr, s_va = 0.0, 0.0\n",
    "\n",
    "    for cap, n in zip(df_caps[\"capture_id\"].tolist(), df_caps[\"n_flows\"].tolist()):\n",
    "        n = float(n)\n",
    "        if s_tr < target_train:\n",
    "            tr.append(cap); s_tr += n\n",
    "        elif s_va < target_val:\n",
    "            va.append(cap); s_va += n\n",
    "        else:\n",
    "            te.append(cap)\n",
    "\n",
    "    return tr, va, te\n",
    "\n",
    "vpn_tr, vpn_va, vpn_te = greedy_split_by_flows(vpn_caps)\n",
    "non_tr, non_va, non_te = greedy_split_by_flows(non_caps)\n",
    "\n",
    "train_caps = sorted(vpn_tr + non_tr)\n",
    "val_caps   = sorted(vpn_va + non_va)\n",
    "test_caps  = sorted(vpn_te + non_te)\n",
    "\n",
    "# --- SAFETY CHECKS ---\n",
    "train_set = set(train_caps)\n",
    "val_set   = set(val_caps)\n",
    "test_set  = set(test_caps)\n",
    "\n",
    "assert train_set.isdisjoint(val_set)\n",
    "assert train_set.isdisjoint(test_set)\n",
    "assert val_set.isdisjoint(test_set)\n",
    "\n",
    "all_split = train_set | val_set | test_set\n",
    "all_caps  = set(flows[\"capture_id\"].unique().tolist())\n",
    "\n",
    "assert all_split == all_caps, \"Some captures are missing from splits!\"\n",
    "\n",
    "def flow_counts(caps):\n",
    "    sub = flows[flows[\"capture_id\"].isin(caps)]\n",
    "    return sub[\"label\"].value_counts().to_dict(), len(sub)\n",
    "\n",
    "tr_counts, tr_flows = flow_counts(train_caps)\n",
    "va_counts, va_flows = flow_counts(val_caps)\n",
    "te_counts, te_flows = flow_counts(test_caps)\n",
    "\n",
    "# Save\n",
    "train_path.write_text(\"\\n\".join(train_caps), encoding=\"utf-8\")\n",
    "val_path.write_text(\"\\n\".join(val_caps), encoding=\"utf-8\")\n",
    "test_path.write_text(\"\\n\".join(test_caps), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Captures total:\", len(all_caps))\n",
    "print(\"Captures Train/Val/Test:\", len(train_caps), len(val_caps), len(test_caps))\n",
    "print(\"Flows Train:\", tr_flows, \"class_counts:\", tr_counts)\n",
    "print(\"Flows Val:  \", va_flows, \"class_counts:\", va_counts)\n",
    "print(\"Flows Test: \", te_flows, \"class_counts:\", te_counts)\n",
    "print(\"Saved:\", train_path, val_path, test_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
